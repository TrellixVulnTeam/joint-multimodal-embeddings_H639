@inProceedings{yu2019mcan,
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  title = {Deep Modular Co-Attention Networks for Visual Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {6281--6290},
  year = {2019}
}

@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13--23},
  year={2019}
}

@InProceedings{lu2020multitask,
author = {Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
title = {12-in-1: Multi-Task Vision and Language Representation Learning},
booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{antol2015vqa,
author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C. and Parikh, Devi},
title = {VQA: Visual Question Answering},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
} 

@incollection{malinowski2014vqa,
title = {A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input},
author = {Malinowski, Mateusz and Fritz, Mario},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {1682--1690},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5411-a-multi-world-approach-to-question-answering-about-real-world-scenes-based-on-uncertain-input.pdf}
}

@inproceedings{zhao2018vqa,
  title     = {Open-Ended Long-form Video Question Answering via Adaptive Hierarchical Reinforced Networks},
  author    = {Zhou Zhao and Zhu Zhang and Shuwen Xiao and Zhou Yu and Jun Yu and Deng Cai and Fei Wu and Yueting Zhuang},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {3683--3689},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/512},
  url       = {https://doi.org/10.24963/ijcai.2018/512},
}

@article{zellers2019vcr,
  title={From Recognition to Cognition: Visual Commonsense Reasoning},
  author={Rowan Zellers and Yonatan Bisk and Ali Farhadi and Yejin Choi},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={6713-6724}
}

@inproceedings{wang2016retrieval,
  author={L. {Wang} and Y. {Li} and S. {Lazebnik}},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, title={Learning Deep Structure-Preserving Image-Text Embeddings}, 
  year={2016},
  volume={},
  number={},
  pages={5005-5013},
}

@article{xie2019entailment,
  author    = {Ning Xie and
               Farley Lai and
               Derek Doran and
               Asim Kadav},
  title     = {Visual Entailment: {A} Novel Task for Fine-Grained Image Understanding},
  journal   = {CoRR},
  volume    = {abs/1901.06706},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.06706},
  archivePrefix = {arXiv},
  eprint    = {1901.06706},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-06706.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{agrawal12018gvqa,
author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
title = {Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@inproceedings{yang2016vqa,
	author={Z. {Yang} and X. {He} and J. {Gao} and L. {Deng} and A. {Smola}},	
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 	
	title={Stacked Attention Networks for Image Question Answering}, 	
	year={2016},	
	volume={},	
	number={},	
	pages={21-29},
}

@article{hudson2018mac,
  title={Compositional Attention Networks for Machine Reasoning},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{hudson2019gqa,
  title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@article{goyal2017vqa2,
  title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},
  author={Yash Goyal and Tejas Khot and Douglas Summers-Stay and Dhruv Batra and D. Parikh},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={6325-6334}
}

@article{yu2018beyond,
  title={Beyond Bilinear: Generalized Multimodal Factorized High-Order Pooling for Visual Question Answering},
  author={Yu, Zhou and Yu, Jun and Xiang, Chenchao and Fan, Jianping and Tao, Dacheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={29},
  number={12},
  pages={5947--5959},
  year={2018}
}

@InProceedings{Nguyen_2018_CVPR,
author = {Nguyen, Duy-Kien and Okatani, Takayuki},
title = {Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@incollection{ban,
title = {Bilinear Attention Networks},
author = {Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {1564--1574},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf}
}

@incollection{transformers,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{relu,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807â€“814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@INPROCEEDINGS{residual,  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Deep Residual Learning for Image Recognition},   year={2016},  volume={},  number={},  pages={770-778},}

@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735â€“1780},
numpages = {46}
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@InProceedings{Anderson_2018_CVPR,
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@incollection{faster_rcnn,
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {91--99},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{mogadala2019vqa-survey,
	author    = {Aditya Mogadala and
	Marimuthu Kalimuthu and
	Dietrich Klakow},
	title     = {Trends in Integration of Vision and Language Research: {A} Survey
	of Tasks, Datasets, and Methods},
	journal   = {CoRR},
	volume    = {abs/1907.09358},
	year      = {2019},
	url       = {http://arxiv.org/abs/1907.09358},
	archivePrefix = {arXiv},
	eprint    = {1907.09358},
	timestamp = {Tue, 30 Jul 2019 12:52:26 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1907-09358.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{mikolov2013efficient,
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	biburl = {https://www.bibsonomy.org/bibtex/28b132b4b7e82cfb538fd462887ba98b8/florianpircher},
	description = {Efficient Estimation of Word Representations in Vector Space},
	interhash = {e92df552b17e9f952226a893b84ad739},
	intrahash = {8b132b4b7e82cfb538fd462887ba98b8},
	keywords = {final thema:sequence_labeling word_embedding},
	note = {cite arxiv:1301.3781},
	timestamp = {2018-11-27T09:35:06.000+0100},
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	year = {2013}
}

@inproceedings{plummer2015vdg,
	author = {Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
	title = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models},
	year = {2015},
	isbn = {9781467383912},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://doi.org/10.1109/ICCV.2015.303},
	doi = {10.1109/ICCV.2015.303},
	booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
	pages = {2641â€“2649},
	numpages = {9},
	series = {ICCV '15}
}

@inproceedings{fitzgerald2013learning,
	title = {Learning Distributions over Logical Forms for Referring Expression Generation},
	author = {FitzGerald, Nicholas  and
	Artzi, Yoav  and
	Zettlemoyer, Luke},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	year = {2013},
	address = {Seattle, Washington, USA},
	publisher = {Association for Computational Linguistics},
	url = {https://www.aclweb.org/anthology/D13-1197},
	pages = {1914--1925},
}

@inproceedings{nagaraja2016modeling,
author={Nagaraja, Varun K.
and Morariu, Vlad I.
and Davis, Larry S.},
editor={Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
title={Modeling Context Between Objects for Referring Expression Understanding},
booktitle={Computer Vision -- ECCV 2016},
year={2016},
publisher={Springer International Publishing},
address={Cham},
pages={792--807},
abstract={Referring expressions usually describe an object using properties of the object and relationships of the object with other objects. We propose a technique that integrates context between objects to understand referring expressions. Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region. The context regions are discovered using multiple-instance learning (MIL) since annotations for context objects are generally not available for training. We utilize max-margin based MIL objective functions for training the LSTM. Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties. We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region.},
isbn={978-3-319-46493-0}
}

@inproceedings{yu2016modeling,
author={Yu, Licheng
and Poirson, Patrick
and Yang, Shan
and Berg, Alexander C.
and Berg, Tamara L.},
editor={Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
title={Modeling Context in Referring Expressions},
booktitle={Computer Vision -- ECCV 2016},
year={2016},
publisher={Springer International Publishing},
address={Cham},
pages={69--85},
abstract={Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from https://github.com/lichengunc/refer), shows the advantages of our methods for both referring expression generation and comprehension.},
isbn={978-3-319-46475-6}
}

@inproceedings{johnson2017clevr,
  title={CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens
          and Fei-Fei, Li and Zitnick, C Lawrence and Girshick, Ross},
  booktitle={CVPR},
  year={2017}
}

@article {geman2015visual,
	author = {Geman, Donald and Geman, Stuart and Hallonquist, Neil and Younes, Laurent},
	title = {Visual Turing test for computer vision systems},
	volume = {112},
	number = {12},
	pages = {3618--3623},
	year = {2015},
	doi = {10.1073/pnas.1422953112},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/112/12/3618},
	eprint = {https://www.pnas.org/content/112/12/3618.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{manmadhan2020vqa,
	author = {Manmadhan, Sruthy and Kovoor, Binsu},
	year = {2020},
	month = {04},
	pages = {},
	title = {Visual question answering: a state-of-the-art review},
	journal = {Artificial Intelligence Review},
	doi = {10.1007/s10462-020-09832-7}
}

@incollection{krizhevsky2012imagenet,
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems 25},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {1097--1105},
	year = {2012},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{russakovsky2015imagenet,
	Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
	Title = {{ImageNet Large Scale Visual Recognition Challenge}},
	Year = {2015},
	journal   = {International Journal of Computer Vision (IJCV)},
	doi = {10.1007/s11263-015-0816-y},
	volume={115},
	number={3},
	pages={211-252}
}

@inproceedings{zeiler2014visual,
author={Zeiler, Matthew D.
and Fergus, Rob},
editor={Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne},
title={Visualizing and Understanding Convolutional Networks},
booktitle={Computer Vision -- ECCV 2014},
year={2014},
publisher={Springer International Publishing},
address={Cham},
pages={818--833},
abstract={Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
isbn={978-3-319-10590-1}
}

@article{simonyan2015very,
	title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={international conference on learning representations},
	year={2015}
}

@article{szegedy2015going,
  title={Going deeper with convolutions},
  author={Christian Szegedy and W. Liu and Y. Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and D. Erhan and V. Vanhoucke and Andrew Rabinovich},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={1-9}
}

@article{miller1991contextual,
	title={Contextual correlates of semantic similarity},
	author={G. Miller and W. Charles},
	journal={Language and Cognitive Processes},
	year={1991}
}

@inproceedings{peters2018elmo,
    title = {Deep Contextualized Word Representations},
    author = {Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    month = jun,
    year = {2018},
    address = {New Orleans, Louisiana},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-1202},
    doi = {10.18653/v1/N18-1202},
    pages = {2227--2237},
    abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
}

@inproceedings{cho2014gru,
    title = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
    month = oct,
    year = {2014},
    address = {Doha, Qatar},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D14-1179},
    doi = {10.3115/v1/D14-1179},
    pages = {1724--1734},
}

@article{elman1990finding,
title = {Finding structure in time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179 - 211},
year = {1990},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(90)90002-E},
url = {http://www.sciencedirect.com/science/article/pii/036402139090002E},
author = {Jeffrey L. Elman},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.}
}

@article{young2018recent,
  author={T. {Young} and D. {Hazarika} and S. {Poria} and E. {Cambria}},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Recent Trends in Deep Learning Based Natural Language Processing [Review Article]}, 
  year={2018},
  volume={13},
  number={3},
  pages={55-75},
}

@inproceedings{sharma-etal-2018-conceptual,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}


@inproceedings{krishnavisualgenome,
  title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and Bernstein, Michael and Fei-Fei, Li},
  year = {2016},
  url = {https://arxiv.org/abs/1602.07332},
}

@InProceedings{coco,
author="Lin, Tsung-Yi
and Maire, Michael
and Belongie, Serge
and Hays, James
and Perona, Pietro
and Ramanan, Deva
and Doll{\'a}r, Piotr
and Zitnick, C. Lawrence",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Microsoft COCO: Common Objects in Context",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="740--755",
abstract="We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
isbn="978-3-319-10602-1"
}

@inproceedings{kazemzadeh-etal-2014-referitgame,
    title = "{R}efer{I}t{G}ame: Referring to Objects in Photographs of Natural Scenes",
    author = "Kazemzadeh, Sahar  and
      Ordonez, Vicente  and
      Matten, Mark  and
      Berg, Tamara",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1086",
    doi = "10.3115/v1/D14-1086",
    pages = "787--798",
}

@INPROCEEDINGS{maorefcoco,  author={J. {Mao} and J. {Huang} and A. {Toshev} and O. {Camburu} and A. {Yuille} and K. {Murphy}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Generation and Comprehension of Unambiguous Object Descriptions},   year={2016},  volume={},  number={},  pages={11-20},}

@InProceedings{zhu2016cvpr,
  title = {{Visual7W: Grounded Question Answering in Images}},
  author = {Yuke Zhu and Oliver Groth and Michael Bernstein and Li Fei-Fei},
  booktitle = {{IEEE Conference on Computer Vision and Pattern Recognition}},
  year = 2016,
}

@inproceedings{guesswhat_game,
author = {Harm de Vries and Florian Strub and Sarath Chandar and Olivier Pietquin and Hugo Larochelle and Aaron C. Courville},
title = {GuessWhat?! Visual object discovery through multi-modal dialogue},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2017}
}

@inproceedings{suhr-etal-2019-corpus,
    title = "A Corpus for Reasoning about Natural Language Grounded in Photographs",
    author = "Suhr, Alane  and
      Zhou, Stephanie  and
      Zhang, Ally  and
      Zhang, Iris  and
      Bai, Huajun  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1644",
    doi = "10.18653/v1/P19-1644",
    pages = "6418--6428",
    abstract = "We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.",
}

@misc{xie2018visual,
    title={Visual Entailment Task for Visually-Grounded Language Learning},
    author={Ning Xie and Farley Lai and Derek Doran and Asim Kadav},
    year={2018},
    eprint={1811.10582},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}