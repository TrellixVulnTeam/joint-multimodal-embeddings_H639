@inProceedings{yu2019mcan,
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  title = {Deep Modular Co-Attention Networks for Visual Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {6281--6290},
  year = {2019}
}

@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13--23},
  year={2019}
}

@InProceedings{lu2020multitask,
author = {Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
title = {12-in-1: Multi-Task Vision and Language Representation Learning},
booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@article{antol2015vqa,
  author    = {Stanislaw Antol and
               Aishwarya Agrawal and
               Jiasen Lu and
               Margaret Mitchell and
               Dhruv Batra and
               C. Lawrence Zitnick and
               Devi Parikh},
  title     = {{VQA:} Visual Question Answering},
  journal   = {CoRR},
  volume    = {abs/1505.00468},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.00468},
  archivePrefix = {arXiv},
  eprint    = {1505.00468},
  timestamp = {Mon, 13 Aug 2018 16:48:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/AntolALMBZP15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{malinowski2014vqa,
  author    = {Mateusz Malinowski and
               Mario Fritz},
  title     = {A Multi-World Approach to Question Answering about Real-World Scenes
               based on Uncertain Input},
  journal   = {CoRR},
  volume    = {abs/1410.0210},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.0210},
  archivePrefix = {arXiv},
  eprint    = {1410.0210},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MalinowskiF14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kim2018vqa,
  author    = {Jin{-}Hwa Kim and
               Jaehyun Jun and
               Byoung{-}Tak Zhang},
  title     = {Bilinear Attention Networks},
  journal   = {CoRR},
  volume    = {abs/1805.07932},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.07932},
  archivePrefix = {arXiv},
  eprint    = {1805.07932},
  timestamp = {Mon, 13 Aug 2018 16:47:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-07932.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhao2018vqa,
  title     = {Open-Ended Long-form Video Question Answering via Adaptive Hierarchical Reinforced Networks},
  author    = {Zhou Zhao and Zhu Zhang and Shuwen Xiao and Zhou Yu and Jun Yu and Deng Cai and Fei Wu and Yueting Zhuang},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {3683--3689},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/512},
  url       = {https://doi.org/10.24963/ijcai.2018/512},
}

@article{zellers2018vcr,
  author    = {Rowan Zellers and
               Yonatan Bisk and
               Ali Farhadi and
               Yejin Choi},
  title     = {From Recognition to Cognition: Visual Commonsense Reasoning},
  journal   = {CoRR},
  volume    = {abs/1811.10830},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.10830},
  archivePrefix = {arXiv},
  eprint    = {1811.10830},
  timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-10830.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2015retrieval,
  author    = {Liwei Wang and
               Yin Li and
               Svetlana Lazebnik},
  title     = {Learning Deep Structure-Preserving Image-Text Embeddings},
  journal   = {CoRR},
  volume    = {abs/1511.06078},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06078},
  archivePrefix = {arXiv},
  eprint    = {1511.06078},
  timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WangLL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{xie2019entailment,
  author    = {Ning Xie and
               Farley Lai and
               Derek Doran and
               Asim Kadav},
  title     = {Visual Entailment: {A} Novel Task for Fine-Grained Image Understanding},
  journal   = {CoRR},
  volume    = {abs/1901.06706},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.06706},
  archivePrefix = {arXiv},
  eprint    = {1901.06706},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-06706.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{agrawal12018gvqa,
  author    = {Aishwarya Agrawal and
               Dhruv Batra and
               Devi Parikh and
               Aniruddha Kembhavi},
  title     = {Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question
               Answering},
  journal   = {CoRR},
  volume    = {abs/1712.00377},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.00377},
  archivePrefix = {arXiv},
  eprint    = {1712.00377},
  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-00377.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{anderson2017vqa,
	author    = {Peter Anderson and
	Xiaodong He and
	Chris Buehler and
	Damien Teney and
	Mark Johnson and
	Stephen Gould and
	Lei Zhang},
	title     = {Bottom-Up and Top-Down Attention for Image Captioning and {VQA}},
	journal   = {CoRR},
	volume    = {abs/1707.07998},
	year      = {2017},
	url       = {http://arxiv.org/abs/1707.07998},
	archivePrefix = {arXiv},
	eprint    = {1707.07998},
	timestamp = {Wed, 02 Sep 2020 13:29:09 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/AndersonHBTJGZ17.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yang2016vqa,
	author={Z. {Yang} and X. {He} and J. {Gao} and L. {Deng} and A. {Smola}},	
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 	
	title={Stacked Attention Networks for Image Question Answering}, 	
	year={2016},	
	volume={},	
	number={},	
	pages={21-29},
}

@article{hudson2018mac,
	author    = {Drew A. Hudson and
	Christopher D. Manning},
	title     = {Compositional Attention Networks for Machine Reasoning},
	journal   = {CoRR},
	volume    = {abs/1803.03067},
	year      = {2018},
	url       = {http://arxiv.org/abs/1803.03067},
	archivePrefix = {arXiv},
	eprint    = {1803.03067},
	timestamp = {Mon, 13 Aug 2018 16:46:02 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1803-03067.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hudson2019gqa,
	author    = {Drew A. Hudson and
	Christopher D. Manning},
	title     = {{GQA:} a new dataset for compositional question answering over real-world
	images},
	journal   = {CoRR},
	volume    = {abs/1902.09506},
	year      = {2019},
	url       = {http://arxiv.org/abs/1902.09506},
	archivePrefix = {arXiv},
	eprint    = {1902.09506},
	timestamp = {Tue, 21 May 2019 18:03:36 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1902-09506.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{goyal2016vqa2,
	author    = {Yash Goyal and
	Tejas Khot and
	Douglas Summers{-}Stay and
	Dhruv Batra and
	Devi Parikh},
	title     = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding
	in Visual Question Answering},
	journal   = {CoRR},
	volume    = {abs/1612.00837},
	year      = {2016},
	url       = {http://arxiv.org/abs/1612.00837},
	archivePrefix = {arXiv},
	eprint    = {1612.00837},
	timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/GoyalKSBP16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yu2018beyond,
  title={Beyond Bilinear: Generalized Multimodal Factorized High-Order Pooling for Visual Question Answering},
  author={Yu, Zhou and Yu, Jun and Xiang, Chenchao and Fan, Jianping and Tao, Dacheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={29},
  number={12},
  pages={5947--5959},
  year={2018}
}

@InProceedings{Nguyen_2018_CVPR,
author = {Nguyen, Duy-Kien and Okatani, Takayuki},
title = {Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@incollection{ban,
title = {Bilinear Attention Networks},
author = {Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {1564--1574},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf}
}

@incollection{transformers,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{relu,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@INPROCEEDINGS{residual,  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Deep Residual Learning for Image Recognition},   year={2016},  volume={},  number={},  pages={770-778},}

@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@InProceedings{Anderson_2018_CVPR,
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@incollection{faster_rcnn,
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {91--99},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
